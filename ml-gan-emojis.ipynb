{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# GAN - Generate Emojis\n",
    "\n",
    "This notebook contains the code for preparing our emoji dataset and train an emoji generator using a GAN composite model.\n",
    "\n",
    "The model was initially created from the `How to Develop a GAN for Generating MNIST Handwritten Digits` tutorial from `Jason Brownlee` at [Machine Learning Mastery](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/). The preprocessing, data augmentation and discriminator model save/load for to resume training were added. The model architecture and hyperparameters were also tuned according to the `UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS` research paper ([available on arxiv.org](https://arxiv.org/pdf/1511.06434.pdf))."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, logging\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Display number of available GPUs on the local machine\n",
    "import tensorflow as tf; print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# List locals devices available for TensorFlow\n",
    "from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\n",
    "\n",
    "# Enable debug logs for TensorFlow\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "source": [
    "# 1. Dataset Preprocessing and Data Augmentation\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "The dataset is preprocessed to resize the original images to 48 x 48 and replacing the transparent background by a 'white' one.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "Each image is then fed through a series of data augmentation transformations to generate more training data by applying slight changes such as :\n",
    "\n",
    "* Rotations\n",
    "    * 15 degrees counterclockwise\n",
    "    * 15 degrees clockwise\n",
    "* Translations\n",
    "    * 5 pixels to the LEFT\n",
    "    * 5 pixels to the RIGHT\n",
    "    * 5 pixels UP\n",
    "    * 5 pixels DOWN\n",
    "    * 5 pixels to the TOP LEFT\n",
    "    * 5 pixels to the TOP RIGHT\n",
    "* Horizontal Flip"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/35859140/remove-transparency-alpha-from-any-image-using-pil\n",
    "def remove_transparency(im, bg_colour=(255, 255, 255)):\n",
    "    # Only process if image has transparency (http://stackoverflow.com/a/1963146)\n",
    "    if im.mode in ('RGBA', 'LA') or (im.mode == 'P' and 'transparency' in im.info):\n",
    "\n",
    "        # Need to convert to RGBA if LA format due to a bug in PIL (http://stackoverflow.com/a/1963146)\n",
    "        alpha = im.convert('RGBA').split()[-1]\n",
    "\n",
    "        # Create a new background image of our matt color\n",
    "        # Must be RGBA because paste requires both images have the same format\n",
    "        # (http://stackoverflow.com/a/8720632  and  http://stackoverflow.com/a/9459208)\n",
    "        bg = Image.new(\"RGBA\", im.size, bg_colour + (255,))\n",
    "        bg.paste(im, mask=alpha)\n",
    "        return bg\n",
    "\n",
    "    else:\n",
    "        return im\n",
    "\n",
    "def preprocess_emojis(emojis_directory, fixed_directory):\n",
    "    emojis = list()\n",
    "\n",
    "    # Make sure that the preprocessed images directory exists\n",
    "    if (not os.path.exists(fixed_directory)):\n",
    "        os.makedirs(fixed_directory)\n",
    "    \n",
    "    # Gather all the filenames in the input images directory\n",
    "    files = [f for f in os.listdir(emojis_directory) if os.path.isfile(os.path.join(emojis_directory, f))]\n",
    "\n",
    "    for filename in files:\n",
    "        # Open the input image\n",
    "        image = Image.open(f'{emojis_directory}/{filename}')\n",
    "\n",
    "        # Resize down to 32x32 pixels\n",
    "        image = image.resize((48, 48))\n",
    "\n",
    "        # Apply data augmentation transformations\n",
    "        data_augmentation(image, f'{fixed_directory}/{filename}')\n",
    "\n",
    "        # Remove the transparency and replace alpha channel with 'white'\n",
    "        image = remove_transparency(image)\n",
    "\n",
    "        # Save the processed image\n",
    "        image.save(f'{fixed_directory}/{filename}')\n",
    "\n",
    "def data_augmentation(image, filename):\n",
    "    # Disable the logging for Pillow\n",
    "    pil_logger = logging.getLogger('PIL')\n",
    "    pil_logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Convert image to RGBA to prevent transformations from \n",
    "    # leaving a colored background\n",
    "    if (image.mode != 'RGBA'):\n",
    "        image = image.convert('RGBA')\n",
    "\n",
    "    # NOTE:\n",
    "    # Don't forget to call remove_transparency(...) to replace \n",
    "    # the alpha channel with 'white' before saving the image\n",
    "\n",
    "    # Apply 15 degree rotation to the left\n",
    "    filename = filename.replace('.png', '-da-nrot.png')\n",
    "    remove_transparency(image.copy().rotate(-15)).save(filename)\n",
    "\n",
    "    # Apply 15 degree rotation to the right\n",
    "    filename = filename.replace('.png', '-da-nrot.png')\n",
    "    remove_transparency(image.copy().rotate(15)).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation to the left\n",
    "    filename = filename.replace('.png', '-da-tl.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, -5, 0, 1, 0))).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation to the right\n",
    "    filename = filename.replace('.png', '-da-tr.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, 5, 0, 1, 0))).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation up\n",
    "    filename = filename.replace('.png', '-da-tu.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, 0, 0, 1, 5))).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation down\n",
    "    filename = filename.replace('.png', '-da-td.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, 0, 0, 1, -5))).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation to the top left\n",
    "    filename = filename.replace('.png', '-da-tlu.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, -5, 0, 1, -5))).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation to the top right\n",
    "    filename = filename.replace('.png', '-da-tru.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, 5, 0, 1, -5))).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation to the bottom left\n",
    "    filename = filename.replace('.png', '-da-tld.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, -5, 0, 1, -5))).save(filename)\n",
    "\n",
    "    # Apply 5 pixel translation to the bottom right\n",
    "    filename = filename.replace('.png', '-da-trd.png')\n",
    "    remove_transparency(image.copy().transform(image.size, Image.AFFINE, (1, 0, 5, 0, 1, -5))).save(filename)\n",
    "\n",
    "    # Only flip images that aren't symetrical.\n",
    "    # filename = f'{self.path}/{emoji[\"id\"]}-{emoji[\"name\"]}-{emojiType}-da-flip.png'\n",
    "    # image.copy().transpose(Image.FLIP_LEFT_RIGHT).save(filename)\n",
    "\n",
    "    # Also apply all of the data augmentation on the flipped version.\n",
    "    # ...\n",
    "\n",
    "def load_emojis(directory):\n",
    "    emojis = list()\n",
    "\n",
    "    # Gather all the filenames in the preprocessed images directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "    for filename in files:\n",
    "        # Open the input image\n",
    "        image = Image.open(f'{directory}/{filename}')\n",
    "\n",
    "        # Convert to 'RGB' mode\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # Extract array of pixels from image\n",
    "        pixels = asarray(image)\n",
    "\n",
    "        # Append pixels to list of emojis\n",
    "        emojis.append(pixels)\n",
    "\n",
    "    return asarray(emojis)\n",
    "\n",
    "def plot_emojis(emojis, n_grid_size):\n",
    "    for i in range(n_grid_size * n_grid_size):\n",
    "        pyplot.subplot(n_grid_size, n_grid_size, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(emojis[i])\n",
    "\n",
    "    pyplot.show()\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "dir_emojis = 'emojis'\n",
    "dir_emojis_fixed = 'emojis-fixed'\n",
    "preprocess_emojis(dir_emojis, dir_emojis_fixed)\n",
    "\n",
    "# Save the clean dataset into a compressed file\n",
    "emojis = load_emojis(dir_emojis_fixed)\n",
    "savez_compressed('emojis-dataset.npz', emojis)\n",
    "\n",
    "# Test reloading the compressed dataset\n",
    "data = load('emojis-dataset.npz')\n",
    "emojis = data['arr_0']\n",
    "\n",
    "# Print dataset details and first X images\n",
    "print('Loaded: ', emojis.shape)\n",
    "plot_emojis(emojis, 7)"
   ]
  },
  {
   "source": [
    "# 2. Model Definitions (Discriminator, Generator and GAN)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape=(48, 48, 3), kernel_size=(3, 3)):\n",
    "    model = Sequential()\n",
    "    # Normal\n",
    "    model.add(Conv2D(64, kernel_size, padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # Downsample to 24x24\n",
    "    model.add(Conv2D(128, kernel_size, strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # Downsample to 12x12\n",
    "    model.add(Conv2D(256, kernel_size, strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # Downsample to 6x6\n",
    "    model.add(Conv2D(512, kernel_size, strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # Classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def define_generator(latent_dim, kernel_size=(5, 5)):\n",
    "    # weight initialization\n",
    "    model = Sequential()\n",
    "    # Foundation for 6x6 feature maps\n",
    "    n_nodes = 512 * 6 * 6\n",
    "    model.add(Dense(n_nodes, activation='relu', input_dim=latent_dim))\n",
    "    # model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((6, 6, 512)))\n",
    "    # Upsample to 12x12\n",
    "    model.add(Conv2DTranspose(256, kernel_size, activation='relu', strides=(2, 2), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.2))\n",
    "    # Upsample to 24x24\n",
    "    model.add(Conv2DTranspose(128, kernel_size, activation='relu', strides=(2, 2), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.2))\n",
    "    # Upsample to 48x48\n",
    "    model.add(Conv2DTranspose(64, kernel_size, activation='relu', strides=(2, 2), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.2))\n",
    "    # Output layer at 48x48x3\n",
    "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "def define_gan(d_model, g_model):\n",
    "    # Freeze the discriminator model's weights from the GAN model\n",
    "    d_model.trainable = False\n",
    "    # Connect the discriminator and generator\n",
    "    model = Sequential()\n",
    "    # Add the generator\n",
    "    model.add(g_model)\n",
    "    # Add the discriminator\n",
    "    model.add(d_model)\n",
    "    # Compile the model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    # Unfreeze the discriminator model's weights\n",
    "    d_model.trainable = True\n",
    "    return model"
   ]
  },
  {
   "source": [
    "# 3. Data Generation Functions\n",
    "\n",
    "These functions are used to generate real or fake samples for training our models.\n",
    "\n",
    "* The real samples are picked from our preprocessed and compressed dataset.\n",
    "* The fake samples are generated using the generator model with random points in its latent space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(dataset_file='emojis-dataset.npz'):\n",
    "    data = load(dataset_file)\n",
    "    X = data['arr_0']\n",
    "    # Convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # Scale from [0, 255] to [-1, 1] for the 'tanh' activation function\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # Choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # Retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # Generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # Generate points in latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # Reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # Generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # Predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # Create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "source": [
    "# 4. Model Performance Functions\n",
    "\n",
    "These functions measure, track and display the performance metrics for our models during training. They are also used to save snapshots of our models after a specified number of epochs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(filename, examples, n=10):\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(examples[i])\n",
    "\n",
    "    # Save plot to file\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "\n",
    "def summarize_performance(epoch, d_model, g_model, dataset, latent_dim, n_samples=100, directory='out_dir'):\n",
    "    # Prepare real samples\n",
    "    x_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # Evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(x_real, y_real, verbose=0)\n",
    "    # Prepare fake samples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # Evaluate generator on fake samples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "\n",
    "    # Summarize discriminator performance\n",
    "    print('> Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "\n",
    "    # TODO: Add historical graphs for the loss over time\n",
    "    # TODO: Track total training time\n",
    "\n",
    "    # Save the discriminator model to file\n",
    "    filename = directory + '/e%03d_discriminator.h5' % (epoch+1)\n",
    "    d_model.save(filename, include_optimizer=True)\n",
    "\n",
    "    # Save the generator model to file\n",
    "    filename = directory + '/e%03d_generator.h5' % (epoch+1)\n",
    "    g_model.save(filename)\n",
    "\n",
    "    # Save the emojis samples generated at this epoch\n",
    "    filename = directory + '/emojis_training_e%03d.png' % (epoch+1)\n",
    "    save_plot(filename, x_fake)"
   ]
  },
  {
   "source": [
    "# 5. Training Function "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d_model, g_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch_size=128):\n",
    "    # Ensure that the output directory exists\n",
    "    runs_dir = 'runs'\n",
    "    if (not os.path.exists(runs_dir)):\n",
    "        os.makedirs(runs_dir)\n",
    "\n",
    "    # Count how many child directories are in the 'runs' directory\n",
    "    run_count = sum(os.path.isdir(os.path.join(runs_dir, i)) for i in os.listdir(runs_dir))\n",
    "\n",
    "    # HACK: Do/While Loop\n",
    "    while True:\n",
    "        # Build the output directory name\n",
    "        out_dir = f'{runs_dir}/run-{run_count}'\n",
    "\n",
    "        # Increment until we find an unused directory name\n",
    "        if (os.path.exists(out_dir)):\n",
    "            run_count += 1\n",
    "        else:\n",
    "            # Create the new output directory \n",
    "            os.makedirs(out_dir)\n",
    "            break\n",
    "\n",
    "    # Calculate batch related parameters\n",
    "    half_batch = int(n_batch_size / 2)\n",
    "    batch_per_epoch = int(dataset.shape[0] / n_batch_size)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(batch_per_epoch):\n",
    "            # Get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # Update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "            \n",
    "            # Generate 'fake' samples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # Update discriminator model weghts\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "            # Prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch_size)\n",
    "            # Create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch_size, 1))\n",
    "            # Update the generator via the discriminator's loss\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "            # Summarize loss on this batch\n",
    "            print('> %d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f' %\n",
    "                (i+1, j+1, batch_per_epoch, d_loss1, d_loss2, g_loss))\n",
    "\n",
    "        # Evaluate the model performance\n",
    "        if ((i+1) % 5 == 0):\n",
    "            summarize_performance(i, d_model, g_model, dataset, latent_dim, directory=out_dir)"
   ]
  },
  {
   "source": [
    "# 6. Main Logic\n",
    "\n",
    "This code puts everything together in order to start training our models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Size of the latent space\n",
    "latent_dim = 100\n",
    "\n",
    "# Create the discriminator\n",
    "# d_model = define_discriminator()\n",
    "d_model = load_model('e075_discriminator.h5')\n",
    "\n",
    "# Create the generator\n",
    "# g_model = define_generator(latent_dim)\n",
    "g_model = load_model('e075_generator.h5')\n",
    "\n",
    "# Create the GAN\n",
    "gan_model = define_gan(d_model, g_model)\n",
    "\n",
    "# Load the emojis\n",
    "dataset = load_real_samples()\n",
    "\n",
    "# Train!!!\n",
    "train(d_model, g_model, gan_model, dataset, latent_dim, n_epochs=75)"
   ]
  },
  {
   "source": [
    "# 7. Generate Emojis (Extra!)\n",
    "\n",
    "This code generates batches of emojis using our trained generator. These emojis can be used to explore the latent space of our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emojis(generator_file, latent_dim):\n",
    "    # Make sure that the generated samples folder exists\n",
    "    out_dir = 'runs/generated'\n",
    "    if (not os.path.exists(out_dir)):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Load the generator model\n",
    "    g_model = load_model(generator_file)\n",
    "\n",
    "    for i in range(100):\n",
    "        # Prepare fake samples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples=36)\n",
    "\n",
    "        # Save generate samples\n",
    "        filename = out_dir + '/emojis_e%03d.png' % (i+1)\n",
    "        save_plot(filename, x_fake, n=6)\n",
    "\n",
    "        # TODO: Save the latent space points for later use in our web application!\n",
    "\n",
    "# Generate some emojis!\n",
    "generate_emojis('e075_generator.h5', latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}